<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>neural network2-设置数据和模型 | plWang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="机器学习神经网络" />
  
  
  
  
  <meta name="description" content="本文主要介绍神经网络中数据和模型的一些设置方法。主要包括数据预处理、权重初始化、正则化以及损失函数等内容。">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network2-设置数据和模型">
<meta property="og:url" content="https://plWang.github.io/2016/12/05/Neural Network2-setModel/index.html">
<meta property="og:site_name" content="plWang's Blog">
<meta property="og:description" content="本文主要介绍神经网络中数据和模型的一些设置方法。主要包括数据预处理、权重初始化、正则化以及损失函数等内容。">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/mormalization.png">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/whitening.png">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/batchnorm-train.png">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/batchnorm-test.png">
<meta property="og:updated_time" content="2016-12-05T07:29:28.224Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Network2-设置数据和模型">
<meta name="twitter:description" content="本文主要介绍神经网络中数据和模型的一些设置方法。主要包括数据预处理、权重初始化、正则化以及损失函数等内容。">
<meta name="twitter:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/mormalization.png">
  
    <link rel="alternate" href="/atom.xml" title="plWang&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/css/dialog.css">
  

  
    <link rel="stylesheet" href="/css/header-post.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

  

</head>

<body>
  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="border-width: 0;">
                <p>plWang&#39;s Blog</p>
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="nav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Neural Network2-setModel" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Neural Network2-设置数据和模型
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2016/12/05/Neural Network2-setModel/" class="article-date">
	  <time datetime="2016-12-05T03:43:00.000Z" itemprop="datePublished">2016-12-05</time>
	</a>

      
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要介绍神经网络中数据和模型的一些设置方法。主要包括数据预处理、权重初始化、正则化以及损失函数等内容。</p>
<a id="more"></a>
<h3 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h3><h4 id="1-均值减法（Mean-Subtraction）"><a href="#1-均值减法（Mean-Subtraction）" class="headerlink" title="1. 均值减法（Mean Subtraction）"></a>1. 均值减法（Mean Subtraction）</h4><p>均值减法是最常用的预处理操作。它对数据中的每个特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心迁移到原点。在numpy中，可以通过<code>X -= np.mean(X,axis=0)</code>来实现。在图像中，更常用的是对所哟与像素都减去同一个值，即<code>X -= np.mean(X)</code>，也可以在R、G、B三个通道上分别进行操作。</p>
<h4 id="2-归一化（Normalization）"><a href="#2-归一化（Normalization）" class="headerlink" title="2. 归一化（Normalization）"></a>2. 归一化（Normalization）</h4><p>有两种方法可以进行归一化。第一种是先对数据零中心化，然后每个维度都除以其标准差。其实现代码为<code>X /= np.std(X, axis=0)</code>；第二种是对每个维度都做归一化，使其最大值和最小值分别为+1和-1。在图像处理中，由于所有像素的值都在(0,255)之间，因此一般不需要进行归一化操作。</p>
<p> <img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/mormalization.png" alt="mormalization"></p>
<h4 id="3-PCA和白化（Whitening）"><a href="#3-PCA和白化（Whitening）" class="headerlink" title="3. PCA和白化（Whitening）"></a>3. PCA和白化（Whitening）</h4><p> 白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。其操作代码为`</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">X -= np.mean(X, axis=<span class="number">0</span>) <span class="comment">#对数据进行零中心化</span></div><div class="line">cov = np.dot(X.T, X)/X.shape[<span class="number">0</span>] <span class="comment">#得到数据的协方差矩阵</span></div><div class="line"></div><div class="line">U,S,V = np.linalg.svd(cov)　<span class="comment">#将协方差矩阵进行奇异值分解</span></div><div class="line">Xrot = np.dot(X, U)　<span class="comment">#对数据去相关性</span></div><div class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment">#取前100维</span></div><div class="line"></div><div class="line">Xwhite = Xrot/np.sqrt(S + <span class="number">1e-5</span>) <span class="comment">#白化，除以特征值, 分母中添加一个小常数防止分母为0</span></div></pre></td></tr></table></figure>
<p> <img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/whitening.png" alt="whitening"></p>
<blockquote>
<p><strong>注意！常见错误</strong>：任何预处理策略都只能在训练集数据上进行计算，算法验证完毕后再应用到验证集和测试集上。比如数据均值操作，应当先将数据分为训练/验证/测试集，<strong>只从训练集中求图片的平均值</strong>，然后在各个集（训练/验证/测试集）中的图像再减去这个均值。</p>
</blockquote>
<h3 id="2-权重初始化"><a href="#2-权重初始化" class="headerlink" title="2. 权重初始化"></a>2. 权重初始化</h3><h4 id="1-全零初始化-错误"><a href="#1-全零初始化-错误" class="headerlink" title="1. 全零初始化(错误)"></a>1. <del>全零初始化</del>(错误)</h4><p>在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。</p>
<p><strong>这种做法是错误的。</strong>因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。</p>
<h4 id="2-小随机数初始化"><a href="#2-小随机数初始化" class="headerlink" title="2. 小随机数初始化"></a>2. 小随机数初始化</h4><p>因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始为很小的数值，以此来<strong>打破对称性</strong>。小随机数权重初始化的方法是<code>W = 0.01 * np.random.randn(D, H)</code>。</p>
<p><strong>警告</strong>：然而并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。</p>
<p><strong>使用1/sqrt(n)校准方差</strong>。上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为:<code>w = np.random.randn(n) / sqrt(n)</code>。其中<strong>n</strong>是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。<strong>实践经验证明，这样做可以提高收敛的速度。</strong></p>
<h4 id="3-稀疏初始化-Sparse-Initialization"><a href="#3-稀疏初始化-Sparse-Initialization" class="headerlink" title="3. 稀疏初始化(Sparse Initialization)"></a>3. 稀疏初始化(Sparse Initialization)</h4><p>另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p>
<h4 id="4-偏置-bias-的初始化"><a href="#4-偏置-bias-的初始化" class="headerlink" title="4. 偏置(bias)的初始化"></a>4. 偏置(bias)的初始化</h4><p>通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，但这样是不是总能提高算法性能并不清楚。</p>
<p><strong>实践：</strong>当前的推荐是使用ReLU函数，并且使用<code>w=np.random.randn(n)*sqrt(2.0/n)</code>来进行初始化。</p>
<h4 id="5-批量归一化（Batch-Normalization）"><a href="#5-批量归一化（Batch-Normalization）" class="headerlink" title="5. 批量归一化（Batch Normalization）"></a>5. 批量归一化（Batch Normalization）</h4><p>批量归一化是最近才提出来的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛。其做法是让激活数据在训练开始前经过一个网络，网络处理数据使其服从标准高斯分布。在实现层面，应用这个技巧通常意味着<strong>全连接层（或者卷积层）与激活函数之间添加一个BatchNorm层</strong>。实践中，在神经网络中使用批量归一化已经变得非常常见，使用了批量归一化的网络<strong>对于不好的初始值有更强的鲁棒性</strong>。</p>
<p>Batch Normalization的做法是对于输入的mini-batch数据，求它们在每个维度上的均值和方差<br>$$<br>\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}<br>$$<br>由于分母中方差可能为0，为了防止数值计算不稳定，通常在分母中加一个小的常数$\epsilon$，即<br>$$<br>\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}] + \epsilon}}<br>$$<br>经过批量归一化输出的数据是零均值/单位方差（zero-mean/unit variance）的，但是我们确实需要激活函数接收的都是零均值/单位方差的输入吗？因此在归一化之后需要将数据变换到我们想要的数值范围上，<br>$$<br>y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}<br>$$<br>其中$\gamma, \beta$两个参数是可以通过学习得到的。当$\gamma^{(k)} = \sqrt{Var[x^{(k)}]}, \beta^{(k)}=E[x^{(k)}]$时，数据就可以恢复为原来的输出。</p>
<p><strong>批量归一化算法步骤（训练阶段）</strong></p>
<p> <img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/batchnorm-train.png" alt="batchnorm-train"></p>
<p><strong>批量归一化算法步骤（测试阶段）</strong></p>
<p> <img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/batchnorm-test.png" alt="batchnorm-test"></p>
<p>需要注意的是，在测试阶段，批量归一化的步骤与训练时是不同的，均值和方差不是由输入数据计算得到的，而是直接使用一个在训练阶段得到的固定值（如训练阶段的running average, <code>running_mean = momentum * running_mean + (1-running_mean * sample_mean)</code>）。</p>
<h3 id="3-正则化"><a href="#3-正则化" class="headerlink" title="3. 正则化"></a>3. 正则化</h3><h4 id="1-L2正则化"><a href="#1-L2正则化" class="headerlink" title="1. L2正则化"></a>1. L2正则化</h4><p>L2正则化可能是最常用的正则化方法，它对于每个权重w，向目标函数中增加了一个$\frac{1}{2}\lambda w^2$项。L2正则化可以直观理解为<strong>它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量</strong>。使网络更倾向于使用所有特征，而不是严重依赖于输入特征中的某些小部分特征。</p>
<h4 id="2-L1正则化"><a href="#2-L1正则化" class="headerlink" title="2. L1正则化"></a>2. L1正则化</h4><p>L1正则化是另一个相对常用的正则化方法，它对每一个w，都向目标函数增加一个$\lambda|w|$。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2 w^2$，这被称作Elastic net regularization。L1正则化会让权重向量在最优化的过程中变得<strong>稀疏</strong>。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。<strong>在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</strong></p>
<h4 id="3-最大范式约束-Max-norm-constraints"><a href="#3-最大范式约束-Max-norm-constraints" class="headerlink" title="3. 最大范式约束(Max norm constraints)"></a>3. 最大范式约束(Max norm constraints)</h4><p>另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$|\overrightarrow{w}|_2 &lt; c$这一条件，一般c值取3或者4。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。</p>
<h4 id="4-随机失活-Dropout"><a href="#4-随机失活-Dropout" class="headerlink" title="4. 随机失活(Dropout)"></a>4. 随机失活(Dropout)</h4><p>Dropout是一种简单又及其有效的正则化方法。在训练的时候，dropout的实现方法是让神经元以超参数p的概率被激活或被设置为0。</p>
<p>在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。</p>
<p>1个三层神经网络的普通版随机失活可以用下面的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">""" 普通版随机失活: 不推荐实现 (看下面笔记) """</span></div><div class="line"></div><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="string">""" X中是输入数据 """</span></div><div class="line">  </div><div class="line">  <span class="comment"># 3层neural network的前向传播</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</div><div class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></div><div class="line">  H1 *= U1 <span class="comment"># drop!</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></div><div class="line">  H2 *= U2 <span class="comment"># drop!</span></div><div class="line">  out = np.dot(W3, H2) + b3</div><div class="line">  </div><div class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></div><div class="line">  <span class="comment"># 进行参数更新... (略)</span></div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 前向传播时模型集成</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></div><div class="line">  out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure>
<blockquote>
<p>注意：在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围。这一点非常重要。</p>
</blockquote>
<p>上述操作不好的性质是必须在测试时对激活数据要按照p进行数值范围调整。既然测试性能如此关键，实际更倾向使用<strong>反向随机失活（inverted dropout）</strong>，它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="string">""" </span></div><div class="line">反向随机失活: 推荐实现方式.</div><div class="line">在训练的时候drop和调整数值范围，测试时不做任何事.</div><div class="line">"""</div><div class="line"></div><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 3层neural network的前向传播</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</div><div class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></div><div class="line">  H1 *= U1 <span class="comment"># drop!</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></div><div class="line">  H2 *= U2 <span class="comment"># drop!</span></div><div class="line">  out = np.dot(W3, H2) + b3</div><div class="line"></div><div class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></div><div class="line">  <span class="comment"># 进行参数更新... (略)</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">  <span class="comment"># 前向传播时模型集成</span></div><div class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></div><div class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">  out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure>
<p><strong>偏置正则化。</strong>对偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。</p>
<p><strong>每层正则化。</strong>对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。</p>
<p><strong>实践</strong>：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。p值一般默认设为0.5，也可能在验证集上调参。</p>
<h3 id="4-损失函数"><a href="#4-损失函数" class="headerlink" title="4. 损失函数"></a>4. 损失函数</h3><h4 id="1-Multiclass-SVM-Loss"><a href="#1-Multiclass-SVM-Loss" class="headerlink" title="1. Multiclass SVM Loss"></a>1. Multiclass SVM Loss</h4><p><strong>损失函数</strong><br>$$<br>L_i=\sum_{j \neq y_i}{max(0,s_j-s_{y_i}+1)}<br>$$</p>
<p>这种损失函数的形式叫作<strong>折叶损失(hinge loss)</strong></p>
<p><strong>梯度</strong></p>
<p>对正确分类的行的梯度<br>$$<br>\bigtriangledown_{w_{y_i}}L_i = -(\sum_{j \neq y_i}\mathbb{1}(\omega_j^T x_i - \omega_{y_i}^T x_i + \Delta &gt; 0))x_i<br>$$</p>
<p>对不正确分类的行的梯度</p>
<p>$$<br>\bigtriangledown_{w_j}L_i = \mathbb{1}(\omega_j^T x_i - \omega_{y_i}^T x_i + \Delta &gt; 0)x_i<br>$$</p>
<p>即</p>
<p>$$<br>Q_{i,k} =<br>\begin{cases}<br>\bigtriangledown_{\omega_{y_i}}L_i \quad if \ j=y_i \\<br>\bigtriangledown_{\omega_{j}}L_i \quad otherwise<br>\end{cases}<br>$$</p>
<p>$$<br>\bigtriangledown_\omega L = \frac{1}{m}(Q^TX)^T = \frac{1}{N}X^TQ<br>$$</p>
<h4 id="2-Softmax-Loss"><a href="#2-Softmax-Loss" class="headerlink" title="2. Softmax Loss"></a>2. Softmax Loss</h4><p><strong>损失函数</strong><br>$$<br>L_i = -log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}}) = -f_{y_i} + log(\sum_je^{f_i})<br>$$<br>这种损失函数的形式叫做<strong>交叉熵损失(cross-entropy loss)</strong></p>
<p>设$p_i = \frac{e^{f_{y_i}}}{\sum_je^{fj}}$, 其导数为<br>$$<br>\frac{\partial p_i}{\partial y_j} = \frac{[i=j]e^{y_i}\sum_c e^{y_c} - e^{y_i}e^{y_j}}{\sum_c e^{y_c}} = [i=j]p_i - p_ip_j<br>$$<br>所以损失函数的梯度为<br>$$<br>\frac{\partial L_i}{\partial y_i} = -\frac{1}{p_i} \frac{\partial p_i}{\partial y_i} = -[i=j] + p_j<br>$$<br>其中，符号[]的意义为，[A] is 1 if A is true, and 0 if A is false.</p>

      
    </div>
    <footer class="article-footer">
      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'plwang';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/12/05/Neural Network3-optim/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Neural Network3-参数更新方法
        
      </div>
    </a>
  
  
    <a href="/2016/12/05/NeuralNetwork1-activationFunction/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Neural Network1-activation function</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-数据预处理"><span class="toc-number">1.</span> <span class="toc-text">1. 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-均值减法（Mean-Subtraction）"><span class="toc-number">1.1.</span> <span class="toc-text">1. 均值减法（Mean Subtraction）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-归一化（Normalization）"><span class="toc-number">1.2.</span> <span class="toc-text">2. 归一化（Normalization）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-PCA和白化（Whitening）"><span class="toc-number">1.3.</span> <span class="toc-text">3. PCA和白化（Whitening）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-权重初始化"><span class="toc-number">2.</span> <span class="toc-text">2. 权重初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-全零初始化-错误"><span class="toc-number">2.1.</span> <span class="toc-text">1. 全零初始化(错误)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-小随机数初始化"><span class="toc-number">2.2.</span> <span class="toc-text">2. 小随机数初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-稀疏初始化-Sparse-Initialization"><span class="toc-number">2.3.</span> <span class="toc-text">3. 稀疏初始化(Sparse Initialization)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-偏置-bias-的初始化"><span class="toc-number">2.4.</span> <span class="toc-text">4. 偏置(bias)的初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-批量归一化（Batch-Normalization）"><span class="toc-number">2.5.</span> <span class="toc-text">5. 批量归一化（Batch Normalization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-正则化"><span class="toc-number">3.</span> <span class="toc-text">3. 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-L2正则化"><span class="toc-number">3.1.</span> <span class="toc-text">1. L2正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-L1正则化"><span class="toc-number">3.2.</span> <span class="toc-text">2. L1正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-最大范式约束-Max-norm-constraints"><span class="toc-number">3.3.</span> <span class="toc-text">3. 最大范式约束(Max norm constraints)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-随机失活-Dropout"><span class="toc-number">3.4.</span> <span class="toc-text">4. 随机失活(Dropout)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-损失函数"><span class="toc-number">4.</span> <span class="toc-text">4. 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Multiclass-SVM-Loss"><span class="toc-number">4.1.</span> <span class="toc-text">1. Multiclass SVM Loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Softmax-Loss"><span class="toc-number">4.2.</span> <span class="toc-text">2. Softmax Loss</span></a></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      <div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2013 - 2017 plWang&#39;s Blog All Rights Reserved.</p>
	      <p id="copyRightCn">Wang Penglin hold copyright</p>
	</div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>




  <script src="/js/dialog.js"></script>



<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-90550327-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            plWang&#39;s Blog
          </div>
          <div class="panel-body">
            Copyright © 2017 Wang Penglin All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  

</body>
</html>