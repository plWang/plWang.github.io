<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>cs231n assignment2-part1：image captioning using vanilla rnn and lstm | plWang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="机器学习神经网络深度学习" />
  
  
  
  
  <meta name="description" content="Ａbreif introduction to vanilla RNN and LSTM.">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n Assignment2-Part1：Image Captioning using vanilla RNN and LSTM">
<meta property="og:url" content="https://plWang.github.io/2017/03/10/CS231n assignment3-part1/index.html">
<meta property="og:site_name" content="plWang's Blog">
<meta property="og:description" content="Ａbreif introduction to vanilla RNN and LSTM.">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/RNN-unrolled.png">
<meta property="og:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/LSTM3-var-GRU.png">
<meta property="og:updated_time" content="2017-03-10T07:33:24.479Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS231n Assignment2-Part1：Image Captioning using vanilla RNN and LSTM">
<meta name="twitter:description" content="Ａbreif introduction to vanilla RNN and LSTM.">
<meta name="twitter:image" content="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/RNN-unrolled.png">
  
    <link rel="alternate" href="/atom.xml" title="plWang&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/css/dialog.css">
  

  
    <link rel="stylesheet" href="/css/header-post.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

  

</head>

<body>
  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="border-width: 0;">
                <p>plWang&#39;s Blog</p>
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="nav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-CS231n assignment3-part1" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      CS231n Assignment2-Part1：Image Captioning using vanilla RNN and LSTM
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/03/10/CS231n assignment3-part1/" class="article-date">
	  <time datetime="2017-03-10T07:28:00.000Z" itemprop="datePublished">2017-03-10</time>
	</a>

      
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Ａbreif introduction to vanilla RNN and LSTM.</p>
<a id="more"></a>
<h2 id="1-RNN-Captioning"><a href="#1-RNN-Captioning" class="headerlink" title="1. RNN Captioning"></a>1. RNN Captioning</h2><h3 id="1-1-Data-Preprocessing"><a href="#1-1-Data-Preprocessing" class="headerlink" title="1.1 Data Preprocessing"></a>1.1 Data Preprocessing</h3><ol>
<li>prepocess the data and extract the feature: the features have already been extracted from the fc7 layer of the VGG-16 network pretrained on ImageNet</li>
<li>to cut down process time and memory requirements, reduce the dimensionality of the features from 4096 to 512 (Through PCA)</li>
<li>word2index: each word is assgined a integer id</li>
<li>prepend a special <start> and append a special <end> to the beginning and end of each caption; Rare words are replaced by <unk> (as uknown); captions are of different length, in order to train them at the same time, pad short captions with a special <null> token and don’t compute loss or gradient for <null> token.</null></null></unk></end></start></li>
</ol>
<h3 id="1-2-Vanilla-RNN"><a href="#1-2-Vanilla-RNN" class="headerlink" title="1.2 Vanilla RNN"></a>1.2 Vanilla RNN</h3><p><img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/RNN-unrolled.png" alt="Vanilla RNN"></p>
<h4 id="1-2-1-Hyperparameters"><a href="#1-2-1-Hyperparameters" class="headerlink" title="1.2.1 Hyperparameters:"></a>1.2.1 Hyperparameters:</h4><ul>
<li>input_dim: dimention of features, as D</li>
<li>wordvec_dim: dimention of word vector, as W</li>
<li>hidden_dim: dimention of hidden state, as H</li>
<li>vocabulary size: as V</li>
</ul>
<h4 id="1-2-2-Parameters"><a href="#1-2-2-Parameters" class="headerlink" title="1.2.2 Parameters:"></a>1.2.2 Parameters:</h4><ul>
<li>Wx: input to hidden weight, of shape (WxH)</li>
<li>Wh: hidden to hidden weight, of shape (HxH)</li>
<li>b: Biases, of shape (H,)</li>
<li>W_embed: weight matrix of shape (VxW)</li>
<li>W_proj: hidden to vocabulary weight matrix, of shape (HxV)</li>
<li>b_proj: biases of shape (V,)</li>
</ul>
<h4 id="1-2-3-Inputs"><a href="#1-2-3-Inputs" class="headerlink" title="1.2.3 Inputs:"></a>1.2.3 Inputs:</h4><ul>
<li>features (extracted from CNN), of shape (NxD)</li>
<li>captions (ground truth, for training), of shape (NxT)</li>
</ul>
<h4 id="1-2-4-Training-stage"><a href="#1-2-4-Training-stage" class="headerlink" title="1.2.4 Training stage"></a>1.2.4 Training stage</h4><p><strong>1. compute initial hidden state</strong><br>$$<br>h_0 = FW_{proj} + b_{proj}<br>$$<br>F as feature matrix extracted from CNN.</p>
<p><strong>2. word embedding</strong></p>
<p>transform the words(represented as integers) into word vectors.</p>
<p>(NxT) -&gt; (NxTxV) -&gt; (NxTxW)</p>
<p><strong>3. RNN step forward</strong><br>$$<br>h_t = XW_{x} + h_{t-1}W_h + b<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">  next_h = np.tanh(np.dot(x, Wx) + np.dot(prev_h, Wh) + b)</div><div class="line">  cache = (next_h, x, prev_h, Wh, Wx)</div><div class="line">  <span class="keyword">return</span> next_h, cache</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line">  (next_h, x, prev_h, Wh, Wx) = cache</div><div class="line">  dh = (<span class="number">1</span> - next_h * next_h) * dnext_h</div><div class="line">  dWx = np.dot(x.T, dh)</div><div class="line">  dx = np.dot(dh, Wx.T)</div><div class="line">  dWh = np.dot(prev_h.T, dh)</div><div class="line">  dprev_h = np.dot(dh, Wh.T)</div><div class="line">  db = np.sum(dh, axis=<span class="number">0</span>)</div><div class="line"> </div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div></pre></td></tr></table></figure>
<blockquote>
<p>Hint:<br>$$<br>\frac{\partial tanhx}{\partial x} = 1 - tanh^2x<br>$$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">  N, T, D = x.shape</div><div class="line">  H = h0.shape[<span class="number">1</span>]</div><div class="line">  cache = []</div><div class="line">  h = np.zeros((N,T,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(T):</div><div class="line">      <span class="keyword">if</span> t == <span class="number">0</span>:</div><div class="line">          h[:,t,:], cache_tmp = rnn_step_forward(x[:,t,:], h0, Wx, Wh, b)</div><div class="line">      <span class="keyword">else</span>:</div><div class="line">          h[:,t,:], cache_tmp = rnn_step_forward(x[:,t,:], h[:,t<span class="number">-1</span>,:], Wx, Wh, b)</div><div class="line">      cache.append(cache_tmp)</div><div class="line">          </div><div class="line">  <span class="keyword">return</span> h, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">  (next_h, x, prev_h, Wh, Wx) = cache[<span class="number">0</span>]</div><div class="line">  N, T, H = dh.shape</div><div class="line">  D = x.shape[<span class="number">1</span>]</div><div class="line">  dx = np.zeros((N,T,D))</div><div class="line">  dWx = np.zeros((D,H))</div><div class="line">  dWh = np.zeros((H,H))</div><div class="line">  db = np.zeros(H)</div><div class="line">  dprev_h = np.zeros((N,H))</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(T)):</div><div class="line">      dx_tmp, dprev_h, dWx_tmp, dWh_tmp, db_tmp = rnn_step_backward(dh[:,t,:]+dprev_h, cache[t])</div><div class="line">      dx[:,t,:] += dx_tmp</div><div class="line">      dWx += dWx_tmp</div><div class="line">      dWh += dWh_tmp</div><div class="line">      db += db_tmp</div><div class="line">        </div><div class="line">  dh0 = dprev_h</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure>
<p><strong>4. transform the hidden vector at evert timestep into scores for each word in the vocabulary</strong><br>$$<br>scores = h_tW_{vocab} + b_{vocab}<br>$$<br>ht: (NxTxH) -&gt; scores: (NxTxV)</p>
<p><strong>5. compute softmax loss and gradient, ignoring the points where the output word is <null>, sum the loss over time anbd average them on the mini-batch</null></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporal_softmax_loss</span><span class="params">(x, y, mask, verbose=False)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Inputs:</div><div class="line">  - x: Input scores, of shape (N, T, V)</div><div class="line">  - y: Ground-truth indices, of shape (N, T) where each element is in the range</div><div class="line">       0 &lt;= y[i, t] &lt; V</div><div class="line">  - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not</div><div class="line">    the scores at x[i, t] should contribute to the loss.</div><div class="line"></div><div class="line">  Returns a tuple of:</div><div class="line">  - loss: Scalar giving loss</div><div class="line">  - dx: Gradient of loss with respect to scores x.</div><div class="line">  """</div><div class="line"></div><div class="line">  N, T, V = x.shape</div><div class="line">  </div><div class="line">  x_flat = x.reshape(N * T, V)</div><div class="line">  y_flat = y.reshape(N * T)</div><div class="line">  mask_flat = mask.reshape(N * T)</div><div class="line">  </div><div class="line">  probs = np.exp(x_flat - np.max(x_flat, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>))</div><div class="line">  probs /= np.sum(probs, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">  loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N</div><div class="line">  dx_flat = probs.copy()</div><div class="line">  dx_flat[np.arange(N * T), y_flat] -= <span class="number">1</span></div><div class="line">  dx_flat /= N</div><div class="line">  dx_flat *= mask_flat[:, <span class="keyword">None</span>]</div><div class="line">  </div><div class="line">  <span class="keyword">if</span> verbose: <span class="keyword">print</span> <span class="string">'dx_flat: '</span>, dx_flat.shape</div><div class="line">  </div><div class="line">  dx = dx_flat.reshape(N, T, V)</div><div class="line">  </div><div class="line">  <span class="keyword">return</span> loss, dx</div></pre></td></tr></table></figure>
<h4 id="1-2-5-Test-stage"><a href="#1-2-5-Test-stage" class="headerlink" title="1.2.5 Test stage"></a>1.2.5 Test stage</h4><p>Image captioning models behave very different at training time and test time. At training time, we have access to the ground truth, so we feed groud-truth wordsas input to the RNN at each timestep; But at test time, we sample from the distribution over vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.</p>
<p><strong>1. compute initial hidden state (same as training stage)</strong></p>
<p><strong>2. word embedding</strong></p>
<p>the initial word should be <start>. At each timestep, we embed the current word, pass it to the RNN.</start></p>
<p><strong>3. RNN step forward</strong></p>
<p>Different from training stage, we cannot do the forward at one pass. we have to do step forward in every iteration.</p>
<p><strong>4. use affine transform to compute the scores(same as training stage)</strong></p>
<p><strong>5. select the word with highest score as the next word to feed into RNN next timestep</strong></p>
<h3 id="1-3-LSTM-Long-Short-Term-Memory"><a href="#1-3-LSTM-Long-Short-Term-Memory" class="headerlink" title="1.3 LSTM(Long Short Term Memory)"></a>1.3 LSTM(Long Short Term Memory)</h3><h4 id="1-3-1-The-problem-of-simple-RNN"><a href="#1-3-1-The-problem-of-simple-RNN" class="headerlink" title="1.3.1 The problem of simple RNN"></a>1.3.1 The problem of simple RNN</h4><p>Vanilla RNN can be tough to train on long sequences due to vanishing and exploding gradients casued by the repeated matrix multiplication. LSTMs(Long Short Term Memory) are a special kind of RNN, capable of learning long-term dependencies.</p>
<h4 id="1-3-2-The-structure-of-LSTM"><a href="#1-3-2-The-structure-of-LSTM" class="headerlink" title="1.3.2 The structure of LSTM"></a>1.3.2 The structure of LSTM</h4><p><img src="http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/LSTM3-var-GRU.png" alt="LSTM"></p>
<p>Similar to the vanilla RNN, at each timestep we receive an inout $x_t \in R^D$</p>
<p>and the previous hidden state $h_{t-1} \in R^H$; the LSTM also maintains an H-dimentional <em>cell state</em>, so we also receive the previous cell state $c_{t-1} \in R^H$. </p>
<p>At each timestep we first compute an activation vector $a \in R^{4H}$<br>$$<br>a = W_x x_t + W_h h_{t-1} + b<br>$$<br>we then divide this into four vectors $a_i, a_f, a_o, a_g \in R^H$ where $a_i$ consists of the first H elements of a, $a_f$ is the next H elements of a, etc. We then compute the <strong>input gate i</strong>, <strong>forget gate f</strong>, <strong>output gate o</strong> and <strong>block input g</strong> as<br>$$<br>i = sigmoid(a_i) \\<br>f = sigmoid(a_f) \\<br>o = sigmoid(a_o) \\<br>g = tanh(a_g)<br>$$<br>Finally we compute the next cell state $c_t$ and the next hidden state $h_t$ as<br>$$<br>c_t = f \odot c_{t-1} + i \odot g \\<br>h_t = o \odot tanh(c_t)<br>$$<br>where $\odot$ is the elementwise product of vectors.</p>
<p><strong>Implementation of LSTM step forward anf backward</strong>: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line">  N, H = prev_h.shape</div><div class="line">  a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</div><div class="line">  i = sigmoid(a[:,:H])</div><div class="line">  f = sigmoid(a[:,H:<span class="number">2</span>*H])</div><div class="line">  o = sigmoid(a[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">  g = np.tanh(a[:,<span class="number">3</span>*H:<span class="number">4</span>*H])</div><div class="line">  </div><div class="line">  next_c = f*prev_c + i*g</div><div class="line">  next_h = o*np.tanh(next_c)</div><div class="line">  cache = x, prev_h, prev_c, a, i, f, o, g, Wx, Wh, b, next_h, next_c</div><div class="line">  </div><div class="line">  <span class="keyword">return</span> next_h, next_c, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">  x, prev_h, prev_c, a, i, f, o, g, Wx, Wh, b, next_h, next_c = cache</div><div class="line">  do = dnext_h * np.tanh(next_c)</div><div class="line">  dnext_c += dnext_h * o * (<span class="number">1</span>-np.tanh(next_c)**<span class="number">2</span>)</div><div class="line">  dprev_c = dnext_c * f</div><div class="line">  df = dnext_c * prev_c</div><div class="line">  di = dnext_c * g</div><div class="line">  dg = dnext_c * i</div><div class="line">  da_i = di*(<span class="number">1</span>-i)*i</div><div class="line">  da_f = df*(<span class="number">1</span>-f)*f</div><div class="line">  da_o = do*(<span class="number">1</span>-o)*o</div><div class="line">  da_g = dg*(<span class="number">1</span>-g*g)</div><div class="line">  da = np.hstack((da_i, da_f, da_o, da_g))</div><div class="line">  dprev_h = np.dot(da, Wh.T)</div><div class="line">  dWx = np.dot(x.T, da)</div><div class="line">  dWh = np.dot(prev_h.T, da)</div><div class="line">  db = np.sum(da, axis=<span class="number">0</span>)</div><div class="line">  dx = np.dot(da, Wx.T)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'plwang';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/02/09/cs231n-Assignment2-Part2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CS231n课程作业2-Part2：卷积神经网络的模块化实现</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RNN-Captioning"><span class="toc-number">1.</span> <span class="toc-text">1. RNN Captioning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Data-Preprocessing"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 Data Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Vanilla-RNN"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 Vanilla RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-Hyperparameters"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.2.1 Hyperparameters:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-Parameters"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2.2 Parameters:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-Inputs"><span class="toc-number">1.2.3.</span> <span class="toc-text">1.2.3 Inputs:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-4-Training-stage"><span class="toc-number">1.2.4.</span> <span class="toc-text">1.2.4 Training stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-5-Test-stage"><span class="toc-number">1.2.5.</span> <span class="toc-text">1.2.5 Test stage</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-LSTM-Long-Short-Term-Memory"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 LSTM(Long Short Term Memory)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-The-problem-of-simple-RNN"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.3.1 The problem of simple RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-The-structure-of-LSTM"><span class="toc-number">1.3.2.</span> <span class="toc-text">1.3.2 The structure of LSTM</span></a></li></ol></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      <div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2013 - 2017 plWang&#39;s Blog All Rights Reserved.</p>
	      <p id="copyRightCn">Wang Penglin hold copyright</p>
	</div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>




  <script src="/js/dialog.js"></script>



<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-90550327-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            plWang&#39;s Blog
          </div>
          <div class="panel-body">
            Copyright © 2017 Wang Penglin All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  

</body>
</html>